{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. X_test.shape is (10000,784)\n",
    "# X_test[0].shape is (784,)\n",
    "# X_test_conv = X_test.reshape(-1,28,28.1)\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image,\n",
    "    as illustrated in Figure 1.\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, img_height, img_width, img_depth) representing:\n",
    "        m : image batch size\n",
    "        img_height x img_width : pixel h x w = size of images\n",
    "        img_depth : colors e.g. RGB = 3\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X_pad\n",
    "\n",
    "# GRADED FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(img_patch, filter_W, filter_b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation\n",
    "    of the previous layer.\n",
    "    Arguments:\n",
    "    Note filter size is f x f, with the same depth as the input image\n",
    "    img_patch -- slice of input data of shape (f, f, depth_in)\n",
    "    filter_W -- Weight parameters contained in a window - matrix of shape (f, f, depth_in)\n",
    "    filter_b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"conv_single_step img_patch:\", img_patch.shape)\n",
    "    #print(img_patch)\n",
    "\n",
    "    #print(\"conv_single_step filter_W:\", filter_W.shape)\n",
    "    #print(filter_W)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
    "    s = np.multiply(img_patch, filter_W)\n",
    "\n",
    "    #print(\"conv_single_step s:\", s.shape)\n",
    "    #print(s)\n",
    "\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    #print(\"conv_single_step Z:\", Z)\n",
    "\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z + float(filter_b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z\n",
    "\n",
    "# Note here we're use for loops rather than Python vectorization\n",
    "def conv_forward(input, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    Arguments:\n",
    "    input -- output activations of the previous layer,\n",
    "             numpy array of shape (count, height_in, width_in, depth_in)\n",
    "    W -- Weights, numpy array of shape (f, f, depth_in, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (count, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from input's shape (≈1 line)\n",
    "    (count, height_in, width_in, depth_in) = input.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, depth_in, n_C) = W.shape\n",
    "\n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "\n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((height_in - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((width_in - f + 2 * pad) / stride) + 1\n",
    "\n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((count, n_H, n_W, n_C))\n",
    "    # Define matrix A, output after activation here A = np.array(Z.shape)\n",
    "\n",
    "    # Create input_pad by padding input\n",
    "    input_pad = zero_pad(input, pad)\n",
    "\n",
    "    for i in range(count):                                 # loop over the batch of training examples\n",
    "        example_img = input_pad[i]                     # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Use the corners to define the (3D) slice of example_img (See Hint above the cell). (≈1 line)\n",
    "                    img_patch = example_img[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_single_step(img_patch, W[...,c], b[...,c])\n",
    "                    # Add activation here: A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (count, n_H, n_W, n_C))\n",
    "\n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (input, W, b, hparameters)\n",
    "\n",
    "    # Alternatively return A\n",
    "    return Z, cache\n",
    "\n",
    "# Note call with e.g.\n",
    "# np.random.seed(1)\n",
    "# A_prev = np.random.randn(10,4,4,3)\n",
    "# E.g. for 2 x 2 x 3 filters, x8:\n",
    "# W = np.random.randn(2,2,3,8)\n",
    "# b = np.random.randn(1,1,1,8)\n",
    "# hparameters = {\"pad\" : 2,\n",
    "#               \"stride\": 2}\n",
    "\n",
    "# Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "def conv():\n",
    "    W = np.random.randn(3,3,1,8)\n",
    "    b = np.random.randn(1,1,1,8)\n",
    "    hparams = { \"pad\": 2, \"stride\": 2}\n",
    "    X_train_conv = X_train.reshape(-1,28,28,1)\n",
    "    Z, cache = conv_forward(X_train_conv, W, b, hparams)\n",
    "\n",
    "# print(\"Z's mean =\", np.mean(Z))\n",
    "# print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "# print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n",
    "\n",
    "# Max or Average POOLING layer\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "# Call with e.g. A, cache = pool_forward(A_prev, hparameters, mode = \"max\")\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "\n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "\n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "\n",
    "    for i in range(m):                       # loop over the training examples\n",
    "\n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "\n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "\n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "\n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "# Call with e.g. dA, dW, db = conv_backward(Z, cache_conv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
